{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4809be03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ssd/Code/ArtI/SL\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import modules\n",
    "import datetime\n",
    "\n",
    "import yaml\n",
    "\n",
    "import os\n",
    "\n",
    "# standardize relative filepaths\n",
    "%cd ../\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "\n",
    "def generate(model, input, delimiter, output_tokens, tokenizer, temperature=1.0, type=\"sample\", k=5, p=0.9):\n",
    "    for _ in range(output_tokens):\n",
    "        logits = model.forward(input)\n",
    "\n",
    "        # the higher the average magnitude the more softmax acts like argmax\n",
    "        # the less the average magnitude the more it gives weight to smaller stuff\n",
    "        # does not impact ranking, but sampling\n",
    "        last_logits = logits[0][-1] / temperature\n",
    "\n",
    "        # samples based on the output distribution\n",
    "        # batch 0, last token\n",
    "        match type:\n",
    "            case \"sample\":\n",
    "                index = int(torch.multinomial(\n",
    "                    torch.softmax(last_logits, dim=0), 1))\n",
    "            case \"argmax\":\n",
    "                index = int(torch.argmax(last_logits))\n",
    "            case \"topk\":\n",
    "                topk = torch.topk(last_logits, k)\n",
    "                topk_index = torch.multinomial(\n",
    "                    torch.softmax(topk.values, dim=0), 1)\n",
    "                index = int(topk.indices[topk_index])\n",
    "            # implement nucleus sampling\n",
    "\n",
    "        print(tokenizer.id_to_token(index).replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\").replace(\"âĢĻ\", \"'\"), end=\"\")\n",
    "        print(delimiter, end=\"\")\n",
    "\n",
    "        # append token\n",
    "        token = torch.zeros([1, 1], dtype=torch.int)\n",
    "        token[0][0] = index\n",
    "        input = torch.cat((input, token), dim=1)\n",
    "    return\n",
    "\n",
    "# --- INFERENCE ---\n",
    "\n",
    "# TODO implement Inference\n",
    "# during inference we only compute the embedding of the last token\n",
    "# because of this q (and only q) is only a vector, not a matrix\n",
    "# each token only incorporates past token information (because of masked attention)\n",
    "# so we dont need to recompute the old ones, they wouldnt change\n",
    "# but k and v are still needed to compute the current final embedding\n",
    "\n",
    "# TODO implement KV cache\n",
    "# K and V vectors are the same but one row / column for each new token\n",
    "# KV cache takes about 3 times the model size of memory\n",
    "\n",
    "# ========== Config ==========\n",
    "\n",
    "config_path = \"./SL/configs/tinystories_small_5.yaml\"\n",
    "state_path = \"./SL/checkpoints/tinystories_small_5/0/state.yaml\" # state_path=None will initialize a model\n",
    "\n",
    "# ========== Init ==========\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d|%H:%M:%S\")\n",
    "\n",
    "# open config\n",
    "with open(config_path) as ConfigFile:\n",
    "    c = yaml.safe_load(ConfigFile)\n",
    "\n",
    "# open state config\n",
    "with open(state_path) as StateConfigFile:\n",
    "    sc = yaml.safe_load(StateConfigFile)\n",
    "\n",
    "# env_variables\n",
    "torch.set_default_device(\"cuda\") # do this at every tensor instead\n",
    "torch.manual_seed(c[\"seed\"])\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = Tokenizer.from_file(c[\"tokenizer_path\"])\n",
    "c[\"vocab_size\"] = tokenizer.get_vocab_size()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "        # model\n",
    "        bot = modules.transformer(c)\n",
    "        bot = torch.compile(bot, mode=\"default\")\n",
    "        bot.train()\n",
    "        try:\n",
    "            bot.load_state_dict(torch.load(sc[\"model_path\"], weights_only=True))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Model doesnt exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a858b562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Prompt =========================\n",
      "[DOC]\n",
      "\n",
      "======================== Completion =========================\n",
      "[DOC] Once upon a time, there was a happy dog named Max. Max liked to play outside in the warm sun. One day, Max saw a big, red ball. He wanted to play with it.\n",
      "\n",
      "Max tried to push the ball but it was very heavy. He felt sad. Then he saw a bird. The bird liked the ball too. They both tried to push the ball together. But the ball would not move.\n",
      "\n",
      "Max did not give up. He had an idea. He found a long stick in his yard. Max said, \"I can use this stick to push the ball.\" The bird tried to push the ball. It was still fine, but it did not move. The ball was still on the ball. The bird flew away with the ball. Max was happy, and he and the bird played with the ball.[DOC] Once upon a time, there was a young girl named Mia. Mia had a big sister named Lily."
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "        print(25*\"=\" + \" Prompt \" + 25*\"=\")\n",
    "        Prompt = '[DOC]'\n",
    "        print(Prompt+ \"\\n\")\n",
    "        print(24*\"=\" + \" Completion \" + 25*\"=\")\n",
    "        print(Prompt, end=\"\")\n",
    "        Prompt_in = torch.tensor(tokenizer.encode(Prompt).ids).unsqueeze(0)\n",
    "\n",
    "        # the only thing holding you back from inputting arbitrary size is positional encoding\n",
    "        # maybe compute on the fly for inference\n",
    "        output_tokens = 200\n",
    "        delimiter = \"\"\n",
    "        # higher temperature distributes probability mass more evenly (makes the model more \"creative\")\n",
    "        # the default value is 1\n",
    "        temperature = 1\n",
    "\n",
    "        generate(bot, Prompt_in, delimiter, output_tokens,\n",
    "                tokenizer,  temperature=temperature, type=\"sample\", k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
